{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import bisect\n",
    "import numpy as np\n",
    "import os\n",
    "from typing import Dict, Any, List\n",
    "import torch\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModel\n",
    "\n",
    "from data.dataset import BaseDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(BaseDataset):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "        create your own dataset here.\n",
    "        Rename the class name and the file name with your student number\n",
    "    \n",
    "    Example:\n",
    "    - 20218078_dataset.py\n",
    "        @register_dataset(\"20218078_dataset\")\n",
    "        class MyDataset20218078(BaseDataset):\n",
    "            (...)\n",
    "    \"\"\"\n",
    "\n",
    "    PRETRAINED_MODEL_NAME_OR_PATH = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
    "    MODEL_MAX_LENGTH = 128\n",
    "\n",
    "    @staticmethod\n",
    "    def cumsum(sequences):\n",
    "        r, s = [], 0\n",
    "        for e in sequences:\n",
    "            l = len(e)\n",
    "            r.append(l + s)\n",
    "            s += l\n",
    "        return r\n",
    "\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path: str, # data_path should be a path to the processed features\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.data_path = data_path\n",
    "\n",
    "        self.mimiciii_path = os.path.join(self.data_path, \"mimiciii.pickle\")\n",
    "        self.mimiciv_path  = os.path.join(self.data_path, \"mimiciv.pickle\")\n",
    "        self.eicu_path     = os.path.join(self.data_path, \"eicu.pickle\")\n",
    "\n",
    "        self.mimiciii = pickle.load(open(self.mimiciii_path, \"rb\")) if os.path.exists(self.mimiciii_path) else []\n",
    "        self.mimiciv  = pickle.load(open(self.mimiciv_path, \"rb\")) if os.path.exists(self.mimiciv_path) else []\n",
    "        self.eicu     = pickle.load(open(self.eicu_path, \"rb\")) if os.path.exists(self.eicu_path) else []   \n",
    "        \n",
    "        self.raw_datasets = [self.mimiciii, self.mimiciv, self.eicu]\n",
    "        self.cumulative_sizes = self.cumsum(self.raw_datasets)\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.PRETRAINED_MODEL_NAME_OR_PATH)\n",
    "\n",
    "        self.bos_token_id = self.tokenizer.bos_token_id if self.tokenizer.bos_token_id is not None else self.tokenizer.cls_token_id\n",
    "        self.eos_token_id = self.tokenizer.eos_token_id if self.tokenizer.eos_token_id is not None else self.tokenizer.sep_token_id\n",
    "        self.sep_token_id = self.tokenizer.sep_token_id\n",
    "        self.pad_token_id = self.tokenizer.pad_token_id\n",
    "\n",
    "        self.labs_formats = {\n",
    "            \"mimiciii\": \"{ITEMID}: {VALUE} {VALUEUOM}\",\n",
    "            \"mimiciv\": \"{itemid}: {value} {valueuom}\",\n",
    "            \"eicu\": \"{labname}: {labresult} {labmeasurename}\",\n",
    "        }\n",
    "        self.prescrips_formats = {\n",
    "            \"mimiciii\": \"{DRUG_TYPE} - {DRUG} ({PROD_STRENGTH}): {DOSE_VAL_RX} {DOSE_UNIT_RX}\",\n",
    "            \"mimiciv\": \"{drug_type} - {drug} ({prod_strength}): {dose_val_rx} {dose_unit_rx}\",\n",
    "            \"eicu\": \"{drugname}: {dosage} (frequency: {frequency})\",\n",
    "        }\n",
    "        self.inputs_formats = {\n",
    "            \"mimiciii\": \"{ITEMID}: {AMOUNT} {AMOUNTUOM}\",\n",
    "            \"mimiciv\": \"{itemid}: {amount} {amountuom} (rate: {rate} {rateuom})\",\n",
    "            \"eicu\": \"{drugname} - drugrate: {drugrate} infusionrate: {infusionrate} drugamount: {drugamount} volumeoffluid: {volumeoffluid}\",\n",
    "        }\n",
    "\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Note:\n",
    "            You must return a dictionary here or in collator so that the data loader iterator\n",
    "            yields samples in the form of python dictionary. For the model inputs, the key should\n",
    "            match with the argument of the model's forward() method.\n",
    "            Example:\n",
    "                class MyDataset(...):\n",
    "                    ...\n",
    "                    def __getitem__(self, index):\n",
    "                        (...)\n",
    "                        return {\"data_key\": data, \"label\": label}\n",
    "                \n",
    "                class MyModel(...):\n",
    "                    ...\n",
    "                    def forward(self, data_key, **kwargs):\n",
    "                        (...)\n",
    "                \n",
    "        \"\"\"\n",
    "        if index < 0:\n",
    "            if -index > len(self):\n",
    "                raise ValueError(\"absolute value of index should not exceed dataset length\")\n",
    "            index = len(self) + index\n",
    "        dataset_idx = bisect.bisect_right(self.cumulative_sizes, index)\n",
    "        if dataset_idx == 0:\n",
    "            sample_idx = index\n",
    "        else:\n",
    "            sample_idx = index - self.cumulative_sizes[dataset_idx - 1]\n",
    "        \n",
    "        if dataset_idx == 0:\n",
    "            dataset_name = \"mimiciii\"\n",
    "        elif dataset_idx == 1:\n",
    "            dataset_name = \"mimiciv\"\n",
    "        else:\n",
    "            dataset_name = \"eicu\"\n",
    "\n",
    "        return self.preprocess(self.raw_datasets[dataset_idx][sample_idx], dataset_name)\n",
    "    \n",
    "\n",
    "    def tokenize(self, items: List[Dict[str, Any]], format_str: str):\n",
    "        all_input_ids = [self.bos_token_id]\n",
    "        all_attention_mask = [1]\n",
    "\n",
    "        for item in items:\n",
    "            input_str = format_str.format(**item)\n",
    "            tokenized_inputs = self.tokenizer.encode(input_str, add_special_tokens=False)\n",
    "            all_input_ids.extend(tokenized_inputs)\n",
    "            all_attention_mask.extend([1] * len(tokenized_inputs))\n",
    "\n",
    "            all_input_ids.append(self.sep_token_id)\n",
    "            all_attention_mask.append(1)\n",
    "        \n",
    "        all_input_ids.append(self.eos_token_id)\n",
    "        all_attention_mask.append(1)\n",
    "\n",
    "        # PAD or TRUNCATE\n",
    "        if len(all_input_ids) > self.MODEL_MAX_LENGTH:\n",
    "            all_input_ids = all_input_ids[:self.MODEL_MAX_LENGTH-1]\n",
    "            all_input_ids.append(self.eos_token_id)\n",
    "            all_attention_mask = all_attention_mask[:self.MODEL_MAX_LENGTH]\n",
    "            \n",
    "        else:\n",
    "            all_input_ids.extend([self.pad_token_id] * (self.MODEL_MAX_LENGTH - len(all_input_ids)))\n",
    "            all_attention_mask.extend([0] * (self.MODEL_MAX_LENGTH - len(all_attention_mask)))\n",
    "\n",
    "        input_ids = torch.tensor(all_input_ids, dtype=torch.long)\n",
    "        attention_mask = torch.tensor(all_attention_mask, dtype=torch.long)\n",
    "\n",
    "        return input_ids, attention_mask\n",
    "        \n",
    "\n",
    "    def preprocess(self, sample: Dict[str, Any], dataset_name: str) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Note:\n",
    "            You can implement this method to preprocess the sample before returning it.\n",
    "            This method is called in __getitem__ method.\n",
    "        \"\"\"\n",
    "        icustay_id = sample[\"icustay_id\"] if \"mimic\" in dataset_name else sample[\"patientunitstayid\"]\n",
    "        label = sample[\"label\"]\n",
    "        intime = sample[\"intime\"] if \"mimic\" in dataset_name else \"\"\n",
    "        \n",
    "        events: List[str, Any] = sample[\"data\"]\n",
    "\n",
    "        all_input_ids = []\n",
    "        all_attention_mask = []\n",
    "\n",
    "        if dataset_name in [\"mimiciii\", \"mimiciv\"]:\n",
    "            for event in events:\n",
    "                if \"time\" in event:\n",
    "                    time = event[\"time\"]\n",
    "                else:\n",
    "                    time = None\n",
    "\n",
    "                # Padding is done in the tokenize function\n",
    "                # Therefore, all the input_ids and attention_mask should have the same length\n",
    "                if \"labs\" in event and len(event[\"labs\"]) > 0:\n",
    "                    input_ids, attention_mask = self.tokenize(event[\"labs\"], self.labs_formats[dataset_name])\n",
    "                    all_input_ids.append(input_ids)\n",
    "                    all_attention_mask.append(attention_mask)\n",
    "\n",
    "                if \"prescrips\" in event and len(event[\"prescrips\"]) > 0:\n",
    "                    input_ids, attention_mask = self.tokenize(event[\"prescrips\"], self.prescrips_formats[dataset_name])\n",
    "                    all_input_ids.append(input_ids)\n",
    "                    all_attention_mask.append(attention_mask)\n",
    "                \n",
    "                if \"inputs\" in event and len(event[\"inputs\"]) > 0:\n",
    "                    input_ids, attention_mask = self.tokenize(event[\"inputs\"], self.inputs_formats[dataset_name])\n",
    "                    all_input_ids.append(input_ids)\n",
    "                    all_attention_mask.append(attention_mask)\n",
    "        \n",
    "        elif dataset_name == \"eicu\":\n",
    "\n",
    "            if \"labs\" in events and len(events[\"labs\"]) > 0:\n",
    "                input_ids, attention_mask = self.tokenize(events[\"labs\"], self.labs_formats[dataset_name])\n",
    "                all_input_ids.append(input_ids)\n",
    "                all_attention_mask.append(attention_mask)\n",
    "\n",
    "            if \"prescrips\" in events and len(events[\"prescrips\"]) > 0:\n",
    "                input_ids, attention_mask = self.tokenize(events[\"prescrips\"], self.prescrips_formats[dataset_name])\n",
    "                all_input_ids.append(input_ids)\n",
    "                all_attention_mask.append(attention_mask)\n",
    "            \n",
    "            if \"inputs\" in events and len(events[\"inputs\"]) > 0:\n",
    "                input_ids, attention_mask = self.tokenize(events[\"inputs\"], self.inputs_formats[dataset_name])\n",
    "                all_input_ids.append(input_ids)\n",
    "                all_attention_mask.append(attention_mask)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": torch.stack(all_input_ids),\n",
    "            \"attention_mask\": torch.stack(all_attention_mask),\n",
    "            \"label\": label,\n",
    "            \"intime\": intime,\n",
    "            \"icustay_id\": icustay_id,\n",
    "        }\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.cumulative_sizes[-1]\n",
    "\n",
    "    def collator(self, samples):\n",
    "        \"\"\"Merge a list of samples to form a mini-batch.\n",
    "        \n",
    "        Args:\n",
    "            samples (List[dict]): samples to collate\n",
    "        \n",
    "        Returns:\n",
    "            dict: a mini-batch suitable for forwarding with a Model\n",
    "        \n",
    "        Note:\n",
    "            You can use it to make your batch on your own such as outputting padding mask together.\n",
    "            Otherwise, you don't need to implement this method.\n",
    "        \"\"\"\n",
    "\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MyDataset(\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "182997"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[34987, 81146, 182997]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.cumulative_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1851, 22392,  1604,   131,  1489,  1143,  4426,   120,   181,\n",
       "            102,  1851,  1604,  1604,  1477,   131,  1572,  1143,  4426,   120,\n",
       "            181,   102,  1851,  1604,  1580,  1495,   131,   129,   119,   122,\n",
       "          17713,   120,   173,  1233,   102,  1851, 21500,  1477,   131, 11523,\n",
       "           1143,  4426,   120,   181,   102,  1851,  1580, 11964,   131,   122,\n",
       "            119,   123, 17713,   120,   173,  1233,   102,  1851,  1580, 22639,\n",
       "            131, 16308, 17713,   120,   173,  1233,   102,  1851,  1580, 16480,\n",
       "            131,   122,   119,   130, 17713,   120,   173,  1233,   102,  1851,\n",
       "           1580, 20829,   131,   125,   119,   130, 17713,   120,   173,  1233,\n",
       "            102,  1851,  1580,  1559,  1475,   131,   125,   119,   130,  1143,\n",
       "           4426,   120,   181,   102,  1851,  1580,  1604,  1495,   131, 17048,\n",
       "           1143,  4426,   120,   181,   102, 26177,  1568,  1545,   131,  1627,\n",
       "          17713,   120,   173,  1233,   102,  4062, 17175,   102],\n",
       "         [  101, 27445, 18202,   131,  1512,   119,   123,   110,   102, 27445,\n",
       "          26303,   131,   122,   119,   123,  9468,  1179,   102, 27445,  1559,\n",
       "           1527,   131,  1367,   119,   129, 14516,  1665,   102, 27445, 26253,\n",
       "            131,  2724,   119,   125, 14516,  1665,   102,   102,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0],\n",
       "         [  101,  1851, 18910,  1477,   131,   118,   123,  1143,  4426,   120,\n",
       "            181,   102,  1851, 18910,  1527,   131,  1572,  1143,  4426,   120,\n",
       "            181,   102,  1851,  1604, 15292,   131,  3614,  2608,   177,  1403,\n",
       "            102,  1851,  1604, 10973,   131,   128,   119,  3383,  2338,   102,\n",
       "           1851,  1604, 18202,   131,  1853,  2608,   177,  1403,   102,   102,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0],\n",
       "         [  101,  1514,   118,   175, 16931,  3121, 10399,   113,  1406, 17713,\n",
       "          16048,   114,   131,  1406, 17713,   102,  2259,   118,  2495,  5822,\n",
       "           2913,  3170,  1468,   113,  6087,  1306,  1233,  3821,   114,   131,\n",
       "           6087,   182,  1233,   102,  1514,   118,  1899,  4184, 13166,  4063,\n",
       "          27629,  3740,  5498,   113,   126,  1306,  1403,   120,   126,  1306,\n",
       "           1233,  2258,  1233,   114,   131,   126, 17713,   102,  1514,   118,\n",
       "            175,  3452, 18266,  1233,   172,  2875,  5498,   113,  1620,  1306,\n",
       "           1665,  1403,   120,   123,  1306,  1233,  1821,  1643,   114,   131,\n",
       "           1512,   182,  1665,  1403,   102,  2259,   118,  2495,  5822,  2913,\n",
       "           3170,  1468,   113,  6087,  1306,  1233,  3821,   114,   131,  6087,\n",
       "            182,  1233,   102,  1514,   118,   184,  4371,  3293, 18351,   113,\n",
       "           4267, 10606,  1566, 21889, 16048,   114,   113,   126,  1306,  1403,\n",
       "          16048,   114,   131,   123,   119,   126, 17713,   102],\n",
       "         [  101,  1851, 21500,  1477,   131, 10601,  1143,  4426,   120,   181,\n",
       "            102,  1851,  1580, 11964,   131,   122,   119,   127, 17713,   120,\n",
       "            173,  1233,   102,  1851,  1580, 22639,   131, 17576, 17713,   120,\n",
       "            173,  1233,   102,  1851,  1580, 16480,   131,   122,   119,   130,\n",
       "          17713,   120,   173,  1233,   102,  1851,  1580, 20829,   131,   126,\n",
       "            119,   125, 17713,   120,   173,  1233,   102,  1851,  1580,  1559,\n",
       "           1475,   131,   126,   119,   125,  1143,  4426,   120,   181,   102,\n",
       "           1851,  1580,  1604,  1495,   131,  8183,  1143,  4426,   120,   181,\n",
       "            102, 26177,  1568,  1545,   131,  1695, 17713,   120,   173,  1233,\n",
       "            102, 27445, 18202,   131,  1626,   119,   129,   110,   102, 27445,\n",
       "          20581,   131,   128,   119,   128,   176,   120,   173,  1233,   102,\n",
       "          27445, 26303,   131,   122,   119,   123,  9468,  1179,   102, 27445,\n",
       "          19203,   131,  1955,   119,   122,   185,  1403,   102]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1]]),\n",
       " 'label': '[0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 5, 2, -1, -1, 1, 1]',\n",
       " 'intime': '2183-10-09 14:58:18',\n",
       " 'icustay_id': 3024613}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101, 15684, 16337,  ..., 16770, 17713,   102],\n",
       "         [  101, 27445, 26303,  ...,  4426,   120,   102],\n",
       "         [  101,  2259,   118,  ...,   102,  2259,   102],\n",
       "         ...,\n",
       "         [  101,  1851,  1604,  ...,     0,     0,     0],\n",
       "         [  101, 27445, 26253,  ...,     0,     0,     0],\n",
       "         [  101,  1514,   118,  ...,     0,     0,     0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'label': '[0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 2, 0, 0, 0, 2]',\n",
       " 'intime': '2157-04-01 04:21:49',\n",
       " 'icustay_id': 3047424}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[35000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,   185,  3069,   131,  1765,   119,   121, 14516,  1665,   102,\n",
       "          15059,   131, 17048,   119,   121,  2608,  4063,   120,   181,   102,\n",
       "          21177,   131,   124,   119,   130,  2608,  4063,   120,   181,   102,\n",
       "           2495,  5822,  2193,   131,   122,   119,   121,  2608,  4063,   120,\n",
       "            181,   102,   177,  5822,   131,  3236,   119,   122,   110,   102,\n",
       "            185,  1204,   118,  1107,  1197,   131,   122,   119,   122,  6022,\n",
       "            102, 21685, 20636,   131,  9920,   119,   121, 17713,   120,   173,\n",
       "           1233,   102, 21685, 20636,   131, 18030,   119,   121, 17713,   120,\n",
       "            173,  1233,   102,  4885,  9585,   193,  6087,   131, 11202,   119,\n",
       "            121,   180,   120,   182,  1665,  1233,   102,   185,  1204,   131,\n",
       "           1429,   119,   126, 14516,  1665,   102,   172,  1643,  1377,   131,\n",
       "           3140,   119,   121,  2338,   120,   181,   102,   118,   171,  2225,\n",
       "           2155,   131,   121,   119,   123,   110,   102,   102],\n",
       "         [  101, 13316,  9870, 15265,  9016, 15059,  1969, 17713,   185,  1186,\n",
       "            189,  3962,  1665,   131,  1969, 17713,   113,  5625,   131,   186,\n",
       "           2312,   170,  1665,   114,   102,   122,   182,  1233,  2258,  1233,\n",
       "            131,  1119, 17482,  1394, 15059,   113,   185,  1766, 15459,   114,\n",
       "          13837,  2587,   120,   182,  1233,   178,  3361,  1177, 21615,   131,\n",
       "          13837,  2338,   113,  5625,   131,  9468,  1179,   114,   102,  1275,\n",
       "            182,  1233,  2258,  1233,   131, 26825,  1112, 17482,  1204,  1620,\n",
       "           2587,   120,   182,  1233,   188,  1665,  1177, 21615,   131,   121,\n",
       "            118,   129,   126,   113,  5625,   131,   186,  1545,  1324,   188,\n",
       "           1732,   114,   102,   102,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'label': '[0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 2, -1, -1, -1, -1]',\n",
       " 'intime': '',\n",
       " 'icustay_id': 3048832}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['patientunitstayid', 'label', 'data'])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.eicu[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataset[0]\n",
    "input_ids = sample['input_ids'].to(device)\n",
    "attention_mask = sample['attention_mask'].to(device)\n",
    "\n",
    "outputs = model(input_ids, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([31, 128, 768]), torch.Size([31, 768]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['last_hidden_state'].shape, outputs['pooler_output'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 50868 : 9 meq / l [SEP] 50882 : 24 meq / l [SEP] 50893 : 8. 1 mg / dl [SEP] 50902 : 112 meq / l [SEP] 50912 : 1. 3 mg / dl [SEP] 50931 : 113 mg / dl [SEP] 50960 : 2. 0 mg / dl [SEP] 50970 : 5. 5 mg / dl [SEP] 50971 : 4. 1 meq / l [SEP] 50983 : 141 meq / l [SEP] 51006 : 12 mg / dl [SEP] 5114 [SEP]'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.tokenizer.decode(input_ids[0], skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-al",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
